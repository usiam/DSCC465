{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestPredictions(vectorizer, classifier):\n",
    "    X = vectorizer.transform(df_test.text_clean)\n",
    "    return classifier.predict(X)\n",
    "\n",
    "df_test = pd.read_csv(\"cleaning_test_data.csv\")\n",
    "df_train = pd.read_csv(\"cleaned_training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all models and sklearn utilities\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature space generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColumnTransformer with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering!\n",
    "column_trans = ColumnTransformer(\n",
    "    [\n",
    "        ('tfidf_text', TfidfVectorizer(lowercase=True,ngram_range=(1,2)), 'text_clean'),\n",
    "        ('tfidf_bigram', TfidfVectorizer(lowercase=True), 'bigrams'),\n",
    "        ('tfidf_htags', TfidfVectorizer(lowercase=True), 'hashtags'),\n",
    "        ('tfidf_mentions', TfidfVectorizer(lowercase=True), 'mentions'),\n",
    "        # These columns are not needed in the feature space\n",
    "        ('drop_year', 'drop', 'year'),\n",
    "        ('drop_full_text', 'drop', 'full_text'),\n",
    "        ('drop_party_id', 'drop', 'party_id'),\n",
    "    ],\n",
    "    # All other columns will stay as-is in the feature space\n",
    "    remainder='passthrough', n_jobs=-1,\n",
    ")\n",
    "train = column_trans.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CountVectorizer(lowercase=True,ngram_range=(1,2))\n",
    "train = column_trans.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with models\n",
    "\n",
    "For brevity, only the two most successful vectorization techniques and the two most successful classification models have been shown below. As detailed in the report, the `MultinomialNB`, `LinearSVC`, and `GradientBoostingClassifier` were also experimented with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df_train.party_id # labels to learn against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression\n",
    "\n",
    "For Logistic Regression, a grid-search optimization scheme returned a hyperparameter of `C`=10 and `solver`=`liblinear`, albeit for implementation purposes the latter has been swapped out for a stochastic solver -- helps tremendously with runtime concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=10, max_iter=10000, n_jobs=-1, solver='saga')\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(train, train_labels, test_size=0.3)\n",
    "classifier.fit(xtrain, ytrain)\n",
    "score = classifier.score(xtest, ytest)\n",
    "print(f'{classifier}: {score}')\n",
    "\n",
    "y_pred = getTestPredictions(column_trans, classifier)\n",
    "submission_data = np.array([np.arange(y_pred.size), y_pred]).T\n",
    "submission_df = pd.DataFrame(data=submission_data,columns=['id', 'party'])\n",
    "submission_df.to_csv('submission_lrc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RidgeClassifierCV\n",
    "\n",
    "The `cv`=8 parameter corresponds to performing cross-validation while training using the K-folds scheme with 8 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RidgeClassifierCV(C=10, max_iter=10000, n_jobs=-1, solver='saga')\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(train, train_labels, test_size=0.3)\n",
    "classifier.fit(xtrain, ytrain)\n",
    "score = classifier.score(xtest, ytest)\n",
    "print(f'{classifier}: {score}')\n",
    "\n",
    "y_pred = getTestPredictions(transformer, classifier)\n",
    "submission_data = np.array([np.arange(y_pred.size), y_pred]).T\n",
    "submission_df = pd.DataFrame(data=submission_data,columns=['id', 'party'])\n",
    "submission_df.to_csv('submission_rcv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.6.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
