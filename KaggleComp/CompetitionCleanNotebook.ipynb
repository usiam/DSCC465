{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d458dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import html\n",
    "import emoji\n",
    "from string import punctuation as punctuations\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259115ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"congressional_tweet_training_data.csv\") # if training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "402b5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"congressional_tweet_test_data.csv\") # if test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c14213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "downloads = ('punkt', 'averaged_perceptron_tagger', 'wordnet', 'stopwords')\n",
    "for download in downloads:\n",
    "    nltk.download(download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4fc3e",
   "metadata": {},
   "source": [
    "### Interpreting byte-strings and unescaping HTML sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd8a1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tweet):\n",
    "    return html.unescape(eval(tweet).decode('utf-8'))\n",
    "\n",
    "data.full_text = data.full_text.map(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f03062",
   "metadata": {},
   "source": [
    "#### Identifying features to incorporate into learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9283bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regex\n",
    "\n",
    "def count_chars(tweet):\n",
    "    return len(tweet)\n",
    "\n",
    "def count_words(tweet):\n",
    "    return len(tweet.split())\n",
    "\n",
    "def count_htags(htags):\n",
    "    return len(htags.split())\n",
    "\n",
    "def count_mentions(tweet):\n",
    "    mention_matches = re.findall(r'(@[A-Za-z0-9]*)', tweet)\n",
    "    return len(mention_matches)\n",
    "\n",
    "def get_mentions(tweet):\n",
    "    return re.findall(r'(@[A-Za-z0-9]*)', tweet)\n",
    "\n",
    "# Adding some features\n",
    "data['char_count'] = data[\"full_text\"].apply(lambda tweet:count_chars(tweet))\n",
    "data['word_count'] = data[\"full_text\"].apply(lambda tweet:count_words(tweet))\n",
    "data['htag_count'] = data[\"hashtags\"].apply(lambda htags:count_htags(htags))\n",
    "data['mention_count'] = data[\"full_text\"].apply(lambda tweet:count_mentions(tweet))\n",
    "data['mentions'] = data['full_text'].apply(lambda tweet:get_mentions(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b79738",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "\n",
    "The sequence was as follows:\n",
    "\n",
    "- Populating any empty tweet cells\n",
    "- Tokenization\n",
    "- Part-of-speech tagging\n",
    "- Lemmatization\n",
    "- Eliminating stopwords (in English)\n",
    "- Eliminating unwanted tokens, including:\n",
    "    - Emojis\n",
    "    - Hyperlinks\n",
    "    - Numbers\n",
    "    - Twitter noise: RT/VIDEO/AUDIO\n",
    "    - Other HTML characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1efa70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NAN cells with empty strings\n",
    "data.full_text = data.full_text.fillna('').apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4dd507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.full_text = data.full_text.apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "954d54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# nltk to wordnet tag\n",
    "# https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def pos_tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab4772be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 265000/265000 [01:35<00:00, 2763.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization with our in-house POS tagger. How fancy!\n",
    "for index1 in tqdm(range(len(data.full_text))):\n",
    "    tagged_pairs = data.full_text[index1]\n",
    "    for index2 in range(len(tagged_pairs)):\n",
    "        tagged_pair = tagged_pairs[index2]\n",
    "        word, tag = tagged_pair\n",
    "        try:\n",
    "            word = wnl.lemmatize(word, pos_tagger(tag))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        data.full_text[index1][index2] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e9d9f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 265000/265000 [00:13<00:00, 18959.00it/s]\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for index in tqdm(range(len(data.full_text))):\n",
    "    words = data.full_text[index]\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            data.full_text[index].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0896da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.9/site-packages (1.7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Filtering punctuations, emoji, and other undesirable tokens\n",
    "def is_punctuation(string: str) -> bool:    \n",
    "    for ch in string:\n",
    "        if ch in punctuations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_emoji(string: str) -> bool:\n",
    "    for ch in string:\n",
    "        if ch in emoji.UNICODE_EMOJI:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def should_remove(string: str) -> bool:\n",
    "    \"\"\"\n",
    "    Flags a string that is either:\n",
    "        - a number\n",
    "        - less than 2 characters long\n",
    "        - a punctuation\n",
    "        - a link\n",
    "        - an emoji\n",
    "    \"\"\"\n",
    "    return string.isnumeric() or \\\n",
    "            len(string) < 2 or \\\n",
    "            string == 'RT' or \\\n",
    "            string == 'amp' or \\\n",
    "            string == 'http' or \\\n",
    "            string == 'https' or \\\n",
    "            string == 'VIDEO' or \\\n",
    "            string == 'AUDIO' or \\\n",
    "            is_punctuation(string) or \\\n",
    "            re.search(r'^https?:\\/\\/.*[\\r\\n]*', string, flags=re.MULTILINE) is not None or \\\n",
    "            re.search(r'^http?:\\/\\/.*[\\r\\n]*', string, flags=re.MULTILINE) is not None or \\\n",
    "            is_emoji(string)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7bf43676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                        | 0/265000 [00:00<?, ?it/s]/tmp/ipykernel_11/3436983702.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.text_clean[index] = clean_string[:-1]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 265000/265000 [11:06<00:00, 397.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Going through each tweet and cleaning according to the prescriptions above\n",
    "data['text_clean'] = [''] * len(data.full_text)\n",
    "for index in tqdm(range(len(data.full_text))):\n",
    "    words = data.full_text[index]\n",
    "    clean_string = ''\n",
    "    for word in words:\n",
    "        if not should_remove(word):\n",
    "            clean_string += word + ' '\n",
    "    data.text_clean[index] = clean_string[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b215e3",
   "metadata": {},
   "source": [
    "#### Generating bigrams to incorporate as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a09ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cd0a73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['bigrams'] = data[\"text_clean\"].apply(lambda tweet:generate_ngrams(tweet, n_gram=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e764e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_test_data.csv\", index=False) # if test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f45d987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_training_data.csv\", index=False) # if training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
