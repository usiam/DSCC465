{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c02f60",
   "metadata": {},
   "source": [
    "<h1 align='center'>DSCC 465</h1>\n",
    "<h2 align='center'>Assignment 4</h2>\n",
    "<h4 align='right'>Uzair Tahamid Siam</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1c2ef",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcbe64e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:44:54.914750Z",
     "start_time": "2022-02-18T20:44:52.262388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/usiam/opt/anaconda3/lib/python3.9/site-packages (1.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40d5c4",
   "metadata": {},
   "source": [
    "## Downloading additional NLTK packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fbe642a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:44:55.104406Z",
     "start_time": "2022-02-18T20:44:54.917593Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/usiam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/usiam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/usiam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e0dc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510a4b3",
   "metadata": {},
   "source": [
    "<h1 align=center>Q1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f987e",
   "metadata": {},
   "source": [
    "## a) Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ad5780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:44:55.166080Z",
     "start_time": "2022-02-18T20:44:55.105206Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading in data\n",
    "\n",
    "df = pd.read_csv('corona_fake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e0dd7",
   "metadata": {},
   "source": [
    "## b) i) Tokenizing 'text' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b681cd7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:44:58.684237Z",
     "start_time": "2022-02-18T20:44:55.167054Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizing the sentences into separate words\n",
    "tokenized_sentences = df[\"text\"].fillna(\"\").apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43475adc",
   "metadata": {},
   "source": [
    "## ii) POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c89e7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:45:18.372717Z",
     "start_time": "2022-02-18T20:44:58.686032Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding the parts of speech that each word is associated with using pos_tag\n",
    "pos_tag_tokenized_sentences = tokenized_sentences.apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c788f",
   "metadata": {},
   "source": [
    "## iii) Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d646c3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:45:20.892559Z",
     "start_time": "2022-02-18T20:45:18.373544Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# nltk to wordnet tag\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "\n",
    "# nltk to wordnet tagged words \n",
    "wn_tagged = [list(map(lambda x: (x[0], pos_tagger(x[1])), x)) for x in pos_tag_tokenized_sentences]\n",
    "\n",
    "# lemmatized words using wordnetlemmatizer \n",
    "lemmatized_sentences = [[word if not tag else wnl.lemmatize(word, tag) for word, tag in row] for row in wn_tagged]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8730c37",
   "metadata": {},
   "source": [
    "## iv) Filtering Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f6bbb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:45:20.944632Z",
     "start_time": "2022-02-18T20:45:20.893408Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# filtered sentences after removing stop words\n",
    "filtered_sentences = [[w for w in sentence if w not in stop_words] for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4845f2",
   "metadata": {},
   "source": [
    "## v) Removing numbers, words that are shorter than 2, characters, punctuation, links and emojis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaefe997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:24.387166Z",
     "start_time": "2022-02-18T20:45:20.945684Z"
    }
   },
   "outputs": [],
   "source": [
    "### Remove non-words\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# regex expressions to detect valid word\n",
    "def valid_word(string: str)->bool:\n",
    "    return False if re.match(r'[^\\w\\s]', string) or len(string) < 2 or re.match(r'[0-9]+', string) or\\\n",
    "                            bool(re.search('html', string)) or\\\n",
    "                            re.match(r\"http\\S+\", string) or re.match(emoji.get_emoji_regexp(), string) else True\n",
    "\n",
    "final_step = [[w for w in sentence if valid_word(w)] for sentence in filtered_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3820fefb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:24.402749Z",
     "start_time": "2022-02-18T20:46:24.388090Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleaning and inserting into df \n",
    "\n",
    "cleaned = list(map(lambda x: \" \".join(x), final_step))\n",
    "df[\"text_clean\"] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c4acc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b899fc7",
   "metadata": {},
   "source": [
    "<h1 align=center>Q2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138264e",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f3ad7",
   "metadata": {},
   "source": [
    "N-gram is a probabilistic language model used to predict the next item in a sequence whether it be a sentence, DNA, or even a password. If you think of a sentence being covered by a sliding window, then, the *'n'* in n-gram represents the length of that window. For example, if we look at a sentence and look at each word as an item, then using a 1-gram (or unigram) the sentence will be divided into a set containing every word.\n",
    "\n",
    ">Sentence: \"This is a sentence\"\n",
    "\n",
    ">unigram: {{This}, {is}, {a}, {sentence}}\n",
    "\n",
    "Similarly for a bigram or 2-gram,\n",
    "\n",
    ">bi-gram: {{This, is}, {is, a}, {a, sentence}}\n",
    "\n",
    "And so on.\n",
    "\n",
    "One of the key aspects of n-grams is that it is a Markov model in the sense that there is only dependence on the last n-1 words. \n",
    "\n",
    "N-grams are useful in NLP to make sense of words in context so that the meaning of a world can be better understood by comparing the presence of the said word in the presence of words before and after it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e15df",
   "metadata": {},
   "source": [
    "## b) Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33051f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:24.406122Z",
     "start_time": "2022-02-18T20:46:24.403834Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb55e82",
   "metadata": {},
   "source": [
    "## c) i, ii, iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6484d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:27.302441Z",
     "start_time": "2022-02-18T20:46:24.406924Z"
    }
   },
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "\n",
    "# ngram_range=(1,1)\n",
    "countVec1 = CountVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "text_clean_cv1 = countVec1.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,2)\n",
    "countVec2 = CountVectorizer(lowercase=True, ngram_range=(1,2))\n",
    "text_clean_cv2 = countVec2.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,3)\n",
    "countVec3 = CountVectorizer(lowercase=True, ngram_range=(1,3))\n",
    "text_clean_cv3 = countVec3.fit_transform(df.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b79f8b",
   "metadata": {},
   "source": [
    "## d) i, ii, iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8be5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:30.228582Z",
     "start_time": "2022-02-18T20:46:27.303418Z"
    }
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "# ngram_range=(1,1)\n",
    "tfidVec1 = TfidfVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "text_clean_tf1 = tfidVec1.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,2)\n",
    "tfidVec2 = TfidfVectorizer(lowercase=True, ngram_range=(1,2))\n",
    "text_clean_tf2 = tfidVec2.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,3)\n",
    "tfidVec3 = TfidfVectorizer(lowercase=True, ngram_range=(1,3))\n",
    "text_clean_tf3 = tfidVec3.fit_transform(df.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e156c97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62427c",
   "metadata": {},
   "source": [
    "<h1 align=center>Q3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aee95a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:30.231280Z",
     "start_time": "2022-02-18T20:46:30.229469Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e8d06",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c6e85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:25.129298Z",
     "start_time": "2022-02-18T20:46:30.233677Z"
    }
   },
   "outputs": [],
   "source": [
    "# finding accuracy in the countvectorized vectors \n",
    "\n",
    "accuracy_dict_cv = {}\n",
    "\n",
    "# model for fitting\n",
    "lrcv = LogisticRegressionCV(cv=5, random_state=265, max_iter=1000, n_jobs=-1)\n",
    "\n",
    "for i in range(3):\n",
    "    n_gram_range = f\"CV ngram_range=(1,{i+1})\"\n",
    "    eval_statement = f\"train_test_split(text_clean_cv{i+1}, df.label, test_size=0.3, random_state=265)\"\n",
    "    \n",
    "    # splitting into train test\n",
    "    x_train, x_test, y_train, y_test = eval(eval_statement)\n",
    "    \n",
    "    # fitting data using the model\n",
    "    lrcv.fit(x_train, y_train)\n",
    "    \n",
    "    # finding accuracy \n",
    "    accuracy = lrcv.score(x_test, y_test)\n",
    "    \n",
    "    # inserting accuracy and model(as key) into dictionary\n",
    "    accuracy_dict_cv[n_gram_range] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533b9312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:25.133814Z",
     "start_time": "2022-02-18T20:47:25.130663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CV ngram_range=(1,1)': 0.9051724137931034,\n",
       " 'CV ngram_range=(1,2)': 0.9080459770114943,\n",
       " 'CV ngram_range=(1,3)': 0.9166666666666666}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6c2b7",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd1027f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:57.955208Z",
     "start_time": "2022-02-18T20:47:25.134705Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_dict_tfid = {}\n",
    "\n",
    "for i in range(3):\n",
    "    n_gram_range = f\"TFID ngram_range=(1,{i+1})\"\n",
    "    eval_statement = f\"train_test_split(text_clean_tf{i+1}, df.label, test_size=0.3, random_state=265)\"\n",
    "    \n",
    "    # splitting into train test\n",
    "    x_train, x_test, y_train, y_test = eval(eval_statement)\n",
    "    \n",
    "    # fitting data using the model\n",
    "    lrcv.fit(x_train, y_train)\n",
    "    \n",
    "    # finding accuracy \n",
    "    accuracy = lrcv.score(x_test, y_test)\n",
    "    \n",
    "    # inserting accuracy and model(as key) into dictionary\n",
    "    accuracy_dict_tfid[n_gram_range] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae01757c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:57.959986Z",
     "start_time": "2022-02-18T20:47:57.956973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TFID ngram_range=(1,1)': 0.9166666666666666,\n",
       " 'TFID ngram_range=(1,2)': 0.8936781609195402,\n",
       " 'TFID ngram_range=(1,3)': 0.896551724137931}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88efeb",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c50a4545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:57.969484Z",
     "start_time": "2022-02-18T20:47:57.960857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV ngram_range=(1,1)</th>\n",
       "      <td>0.905172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV ngram_range=(1,2)</th>\n",
       "      <td>0.908046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV ngram_range=(1,3)</th>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFID ngram_range=(1,1)</th>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFID ngram_range=(1,2)</th>\n",
       "      <td>0.893678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFID ngram_range=(1,3)</th>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy\n",
       "CV ngram_range=(1,1)    0.905172\n",
       "CV ngram_range=(1,2)    0.908046\n",
       "CV ngram_range=(1,3)    0.916667\n",
       "TFID ngram_range=(1,1)  0.916667\n",
       "TFID ngram_range=(1,2)  0.893678\n",
       "TFID ngram_range=(1,3)  0.896552"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating table to display all the accuracies\n",
    "\n",
    "accuracy_dict = {**accuracy_dict_cv, **accuracy_dict_tfid}\n",
    "accuracy_table = pd.DataFrame.from_dict(accuracy_dict, orient='index', columns=['Accuracy'])\n",
    "accuracy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8119b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c5884",
   "metadata": {},
   "source": [
    "<h1 align=center>Q4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa15bf4",
   "metadata": {},
   "source": [
    "## Comparing the different solvers in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af19b3d",
   "metadata": {},
   "source": [
    "### a) Newton-CG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d3534d",
   "metadata": {},
   "source": [
    "Newton’s method is an improved gradient descent algorithm as it uses a \"*better*\" quadratic function minimisation as it uses the quadratic approximation (i.e. first AND second partial derivatives with the Hessian (the Hessian is a square matrix of second-order partial derivatives of order $n \\times n$). However, it’s computationally expensive because of the Hessian Matrix (i.e. second partial derivatives calculations). And it also attracts to Saddle Points which are common in multivariable optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e29cd8",
   "metadata": {},
   "source": [
    "### b) LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576eb54",
   "metadata": {},
   "source": [
    "This is analogous to Newton’s Method but here the Hessian matrix is approximated using updates specified by gradient evaluations. In other words, it uses an estimation to the inverse Hessian matrix. The term \"L\" which represents \"Limited-memory\" mean it stores only a few vectors that represent the approximation implicitly. When the dataset is small, LBFGS relatively performs the best compared to other methods especially it saves a lot of memory, however there are some important drawbacks such that if it is unsafeguarded, it may not converge to anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a78c7",
   "metadata": {},
   "source": [
    "### c) Liblinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8075e6",
   "metadata": {},
   "source": [
    "The solver uses a Coordinate Descent (CD) algorithm that solves optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. It applies automatic parameter selection i.e. L1 Regularization and is usually a good solver for high dimension datasets and large-scale classification problems. However, one of the drawbacks of Liblinear is that it may get stuck at a non-stationary point if the level curves of a function are not smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4aff27",
   "metadata": {},
   "source": [
    "### d) SAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b400b6",
   "metadata": {},
   "source": [
    "SAG **(Stochastic Average Gradient)** method optimizes the sum of a finite number of smooth convex functions. The SAG method's iteration cost is independent of the number of terms in the sum just like other stochastic gradient (SG) methods. However, since it incorporates a memory of previous gradient values the SAG method achieves a faster convergence rate than other SG methods. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large. Although it is faster, it does have a O(N) memory complexity so for large datasets it may get impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b20e2",
   "metadata": {},
   "source": [
    "### e) SAGA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3001e9",
   "metadata": {},
   "source": [
    "The SAGA solver is a variant of SAG that also supports L1 Regularization instead of just the smooth L2 Regularization like SAG. This makes SAGA a popular choice for sparse multinomial logistic regression and very large dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "author": "me",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
