{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c02f60",
   "metadata": {},
   "source": [
    "<h1 align='center'>DSCC 465</h1>\n",
    "<h2 align='center'>Assignment 4</h2>\n",
    "<h4 align='right'>Uzair Tahamid Siam</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1c2ef",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcbe64e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T01:55:53.142107Z",
     "start_time": "2022-03-24T01:55:51.528563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Users/usiam/miniforge3/envs/tf_env/lib/python3.9/site-packages (1.7.0)\r\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40d5c4",
   "metadata": {},
   "source": [
    "## Downloading additional NLTK packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e6c6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T01:55:53.148323Z",
     "start_time": "2022-03-24T01:55:53.144801Z"
    }
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e71b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T01:55:53.151787Z",
     "start_time": "2022-03-24T01:55:53.149603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbe642a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T01:55:56.149627Z",
     "start_time": "2022-03-24T01:55:55.926652Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/usiam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/usiam/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/usiam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e0dc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510a4b3",
   "metadata": {},
   "source": [
    "<h1 align=center>Q1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f987e",
   "metadata": {},
   "source": [
    "## a) Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ad5780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T01:55:57.921626Z",
     "start_time": "2022-03-24T01:55:57.838517Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading in data\n",
    "\n",
    "df = pd.read_csv('corona_fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254c4cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcaac81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T02:05:24.572877Z",
     "start_time": "2022-03-24T02:05:24.564498Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    words = word_tokenize(text_p)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in filtered_words]\n",
    "    \n",
    "    pos = pos_tag(filtered_words)\n",
    "    \n",
    "    return words, filtered_words, stemmed, pos\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20e5138a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T02:03:34.326990Z",
     "start_time": "2022-03-24T02:03:05.642630Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(token):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns lemmatization of a token\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWordNetLemmatizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:2032\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;66;03m# 0. Check the exception lists\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_exceptions:\n\u001b[0;32m-> 2032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexceptions\u001b[49m:\n\u001b[1;32m   2033\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m filter_forms([form] \u001b[38;5;241m+\u001b[39m exceptions[form])\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = df[\"text\"].fillna(\"\").apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c94d3218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T02:05:58.813615Z",
     "start_time": "2022-03-24T02:05:58.807959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fact'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[2][3][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1decf207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T02:06:07.428428Z",
     "start_time": "2022-03-24T02:06:07.383677Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(token, tag)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(token, tag):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns lemmatization of a token\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWordNetLemmatizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:2008\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_morphy\u001b[39m(\u001b[38;5;28mself\u001b[39m, form, pos, check_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;66;03m# from jordanbg:\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m     \u001b[38;5;66;03m# Given an original string x\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;66;03m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;66;03m#    find a match or you can't go any further\u001b[39;00m\n\u001b[0;32m-> 2008\u001b[0m     exceptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exception_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2009\u001b[0m     substitutions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NN'"
     ]
    }
   ],
   "source": [
    "lemmatize(tokenized_sentences[2][3][0][0], tokenized_sentences[2][3][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5426f37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-24T02:04:15.145199Z",
     "start_time": "2022-03-24T02:04:15.137767Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fact', 'NN'),\n",
       " ('hydroxychloroquine', 'VB'),\n",
       " ('shown', 'VBN'),\n",
       " ('100', 'CD'),\n",
       " ('effective', 'JJ'),\n",
       " ('rate', 'NN'),\n",
       " ('treating', 'VBG'),\n",
       " ('covid19', 'NN'),\n",
       " ('yet', 'RB'),\n",
       " ('democrat', 'VBN'),\n",
       " ('gretchen', 'NN'),\n",
       " ('whitmer', 'NN'),\n",
       " ('threatening', 'VBG'),\n",
       " ('doctors', 'NNS'),\n",
       " ('prescribe', 'VBP'),\n",
       " ('trump', 'NN'),\n",
       " ('something', 'NN'),\n",
       " ('democrats', 'VBZ'),\n",
       " ('okay', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('dying', 'VBG'),\n",
       " ('means', 'NNS'),\n",
       " ('opposing', 'VBG'),\n",
       " ('trump', 'NN'),\n",
       " ('sick', 'NN')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[2][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e0dd7",
   "metadata": {},
   "source": [
    "## b) i) Tokenizing 'text' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b681cd7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:44:58.684237Z",
     "start_time": "2022-02-18T20:44:55.167054Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizing the sentences into separate words\n",
    "tokenized_sentences = df[\"text\"].fillna(\"\").apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43475adc",
   "metadata": {},
   "source": [
    "## ii) POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c89e7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:45:18.372717Z",
     "start_time": "2022-02-18T20:44:58.686032Z"
    }
   },
   "outputs": [],
   "source": [
    "# adding the parts of speech that each word is associated with using pos_tag\n",
    "pos_tag_tokenized_sentences = tokenized_sentences.apply(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c788f",
   "metadata": {},
   "source": [
    "## iii) Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d646c3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:45:20.892559Z",
     "start_time": "2022-02-18T20:45:18.373544Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# nltk to wordnet tag\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "\n",
    "# nltk to wordnet tagged words \n",
    "wn_tagged = [list(map(lambda x: (x[0], pos_tagger(x[1])), x)) for x in pos_tag_tokenized_sentences]\n",
    "\n",
    "# lemmatized words using wordnetlemmatizer \n",
    "lemmatized_sentences = [[word if not tag else wnl.lemmatize(word, tag) for word, tag in row] for row in wn_tagged]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8730c37",
   "metadata": {},
   "source": [
    "## iv) Filtering Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f6bbb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:45:20.944632Z",
     "start_time": "2022-02-18T20:45:20.893408Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# filtered sentences after removing stop words\n",
    "filtered_sentences = [[w for w in sentence if w not in stop_words] for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4845f2",
   "metadata": {},
   "source": [
    "## v) Removing numbers, words that are shorter than 2, characters, punctuation, links and emojis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaefe997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:24.387166Z",
     "start_time": "2022-02-18T20:45:20.945684Z"
    }
   },
   "outputs": [],
   "source": [
    "### Remove non-words\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# regex expressions to detect valid word\n",
    "def valid_word(string: str)->bool:\n",
    "    return False if re.match(r'[^\\w\\s]', string) or len(string) < 2 or re.match(r'[0-9]+', string) or\\\n",
    "                            bool(re.search('html', string)) or\\\n",
    "                            re.match(r\"http\\S+\", string) or re.match(emoji.get_emoji_regexp(), string) else True\n",
    "\n",
    "final_step = [[w for w in sentence if valid_word(w)] for sentence in filtered_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3820fefb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:24.402749Z",
     "start_time": "2022-02-18T20:46:24.388090Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleaning and inserting into df \n",
    "\n",
    "cleaned = list(map(lambda x: \" \".join(x), final_step))\n",
    "df[\"text_clean\"] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c4acc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b899fc7",
   "metadata": {},
   "source": [
    "<h1 align=center>Q2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c138264e",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f3ad7",
   "metadata": {},
   "source": [
    "N-gram is a probabilistic language model used to predict the next item in a sequence whether it be a sentence, DNA, or even a password. If you think of a sentence being covered by a sliding window, then, the *'n'* in n-gram represents the length of that window. For example, if we look at a sentence and look at each word as an item, then using a 1-gram (or unigram) the sentence will be divided into a set containing every word.\n",
    "\n",
    ">Sentence: \"This is a sentence\"\n",
    "\n",
    ">unigram: {{This}, {is}, {a}, {sentence}}\n",
    "\n",
    "Similarly for a bigram or 2-gram,\n",
    "\n",
    ">bi-gram: {{This, is}, {is, a}, {a, sentence}}\n",
    "\n",
    "And so on.\n",
    "\n",
    "One of the key aspects of n-grams is that it is a Markov model in the sense that there is only dependence on the last n-1 words. \n",
    "\n",
    "N-grams are useful in NLP to make sense of words in context so that the meaning of a world can be better understood by comparing the presence of the said word in the presence of words before and after it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e15df",
   "metadata": {},
   "source": [
    "## b) Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33051f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:24.406122Z",
     "start_time": "2022-02-18T20:46:24.403834Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb55e82",
   "metadata": {},
   "source": [
    "## c) i, ii, iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6484d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:27.302441Z",
     "start_time": "2022-02-18T20:46:24.406924Z"
    }
   },
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "\n",
    "# ngram_range=(1,1)\n",
    "countVec1 = CountVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "text_clean_cv1 = countVec1.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,2)\n",
    "countVec2 = CountVectorizer(lowercase=True, ngram_range=(1,2))\n",
    "text_clean_cv2 = countVec2.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,3)\n",
    "countVec3 = CountVectorizer(lowercase=True, ngram_range=(1,3))\n",
    "text_clean_cv3 = countVec3.fit_transform(df.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b79f8b",
   "metadata": {},
   "source": [
    "## d) i, ii, iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8be5e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:30.228582Z",
     "start_time": "2022-02-18T20:46:27.303418Z"
    }
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "# ngram_range=(1,1)\n",
    "tfidVec1 = TfidfVectorizer(lowercase=True, ngram_range=(1,1))\n",
    "text_clean_tf1 = tfidVec1.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,2)\n",
    "tfidVec2 = TfidfVectorizer(lowercase=True, ngram_range=(1,2))\n",
    "text_clean_tf2 = tfidVec2.fit_transform(df.text_clean)\n",
    "\n",
    "\n",
    "# ngram_range=(1,3)\n",
    "tfidVec3 = TfidfVectorizer(lowercase=True, ngram_range=(1,3))\n",
    "text_clean_tf3 = tfidVec3.fit_transform(df.text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e156c97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62427c",
   "metadata": {},
   "source": [
    "<h1 align=center>Q3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aee95a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:46:30.231280Z",
     "start_time": "2022-02-18T20:46:30.229469Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e8d06",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c6e85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:25.129298Z",
     "start_time": "2022-02-18T20:46:30.233677Z"
    }
   },
   "outputs": [],
   "source": [
    "# finding accuracy in the countvectorized vectors \n",
    "\n",
    "accuracy_dict_cv = {}\n",
    "\n",
    "# model for fitting\n",
    "lrcv = LogisticRegressionCV(cv=5, random_state=265, max_iter=1000, n_jobs=-1)\n",
    "\n",
    "for i in range(3):\n",
    "    n_gram_range = f\"CV ngram_range=(1,{i+1})\"\n",
    "    eval_statement = f\"train_test_split(text_clean_cv{i+1}, df.label, test_size=0.3, random_state=265)\"\n",
    "    \n",
    "    # splitting into train test\n",
    "    x_train, x_test, y_train, y_test = eval(eval_statement)\n",
    "    \n",
    "    # fitting data using the model\n",
    "    lrcv.fit(x_train, y_train)\n",
    "    \n",
    "    # finding accuracy \n",
    "    accuracy = lrcv.score(x_test, y_test)\n",
    "    \n",
    "    # inserting accuracy and model(as key) into dictionary\n",
    "    accuracy_dict_cv[n_gram_range] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533b9312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:25.133814Z",
     "start_time": "2022-02-18T20:47:25.130663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CV ngram_range=(1,1)': 0.9051724137931034,\n",
       " 'CV ngram_range=(1,2)': 0.9080459770114943,\n",
       " 'CV ngram_range=(1,3)': 0.9166666666666666}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6c2b7",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd1027f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:57.955208Z",
     "start_time": "2022-02-18T20:47:25.134705Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_dict_tfid = {}\n",
    "\n",
    "for i in range(3):\n",
    "    n_gram_range = f\"TFID ngram_range=(1,{i+1})\"\n",
    "    eval_statement = f\"train_test_split(text_clean_tf{i+1}, df.label, test_size=0.3, random_state=265)\"\n",
    "    \n",
    "    # splitting into train test\n",
    "    x_train, x_test, y_train, y_test = eval(eval_statement)\n",
    "    \n",
    "    # fitting data using the model\n",
    "    lrcv.fit(x_train, y_train)\n",
    "    \n",
    "    # finding accuracy \n",
    "    accuracy = lrcv.score(x_test, y_test)\n",
    "    \n",
    "    # inserting accuracy and model(as key) into dictionary\n",
    "    accuracy_dict_tfid[n_gram_range] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae01757c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:57.959986Z",
     "start_time": "2022-02-18T20:47:57.956973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TFID ngram_range=(1,1)': 0.9166666666666666,\n",
       " 'TFID ngram_range=(1,2)': 0.8936781609195402,\n",
       " 'TFID ngram_range=(1,3)': 0.896551724137931}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict_tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88efeb",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c50a4545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T20:47:57.969484Z",
     "start_time": "2022-02-18T20:47:57.960857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV ngram_range=(1,1)</th>\n",
       "      <td>0.905172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV ngram_range=(1,2)</th>\n",
       "      <td>0.908046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV ngram_range=(1,3)</th>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFID ngram_range=(1,1)</th>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFID ngram_range=(1,2)</th>\n",
       "      <td>0.893678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFID ngram_range=(1,3)</th>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy\n",
       "CV ngram_range=(1,1)    0.905172\n",
       "CV ngram_range=(1,2)    0.908046\n",
       "CV ngram_range=(1,3)    0.916667\n",
       "TFID ngram_range=(1,1)  0.916667\n",
       "TFID ngram_range=(1,2)  0.893678\n",
       "TFID ngram_range=(1,3)  0.896552"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating table to display all the accuracies\n",
    "\n",
    "accuracy_dict = {**accuracy_dict_cv, **accuracy_dict_tfid}\n",
    "accuracy_table = pd.DataFrame.from_dict(accuracy_dict, orient='index', columns=['Accuracy'])\n",
    "accuracy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8119b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c5884",
   "metadata": {},
   "source": [
    "<h1 align=center>Q4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa15bf4",
   "metadata": {},
   "source": [
    "## Comparing the different solvers in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af19b3d",
   "metadata": {},
   "source": [
    "### a) Newton-CG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d3534d",
   "metadata": {},
   "source": [
    "Newton’s method is an improved gradient descent algorithm as it uses a \"*better*\" quadratic function minimisation as it uses the quadratic approximation (i.e. first AND second partial derivatives with the Hessian (the Hessian is a square matrix of second-order partial derivatives of order $n \\times n$). However, it’s computationally expensive because of the Hessian Matrix (i.e. second partial derivatives calculations). And it also attracts to Saddle Points which are common in multivariable optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e29cd8",
   "metadata": {},
   "source": [
    "### b) LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576eb54",
   "metadata": {},
   "source": [
    "This is analogous to Newton’s Method but here the Hessian matrix is approximated using updates specified by gradient evaluations. In other words, it uses an estimation to the inverse Hessian matrix. The term \"L\" which represents \"Limited-memory\" mean it stores only a few vectors that represent the approximation implicitly. When the dataset is small, LBFGS relatively performs the best compared to other methods especially it saves a lot of memory, however there are some important drawbacks such that if it is unsafeguarded, it may not converge to anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a78c7",
   "metadata": {},
   "source": [
    "### c) Liblinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8075e6",
   "metadata": {},
   "source": [
    "The solver uses a Coordinate Descent (CD) algorithm that solves optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. It applies automatic parameter selection i.e. L1 Regularization and is usually a good solver for high dimension datasets and large-scale classification problems. However, one of the drawbacks of Liblinear is that it may get stuck at a non-stationary point if the level curves of a function are not smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4aff27",
   "metadata": {},
   "source": [
    "### d) SAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b400b6",
   "metadata": {},
   "source": [
    "SAG **(Stochastic Average Gradient)** method optimizes the sum of a finite number of smooth convex functions. The SAG method's iteration cost is independent of the number of terms in the sum just like other stochastic gradient (SG) methods. However, since it incorporates a memory of previous gradient values the SAG method achieves a faster convergence rate than other SG methods. It is faster than other solvers for large datasets, when both the number of samples and the number of features are large. Although it is faster, it does have a O(N) memory complexity so for large datasets it may get impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b20e2",
   "metadata": {},
   "source": [
    "### e) SAGA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3001e9",
   "metadata": {},
   "source": [
    "The SAGA solver is a variant of SAG that also supports L1 Regularization instead of just the smooth L2 Regularization like SAG. This makes SAGA a popular choice for sparse multinomial logistic regression and very large dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "author": "me",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
