{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcca640f",
   "metadata": {},
   "source": [
    "<h1 align='center'>DSCC 465</h1>\n",
    "<h2 align='center'>Assignment 3</h2>\n",
    "<h4 align='right'>Uzair Tahamid Siam</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c319fce",
   "metadata": {},
   "source": [
    "<h1 align='center'>Q1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3957a8e",
   "metadata": {},
   "source": [
    "# a)\n",
    "To summarize,\n",
    "\n",
    "the paper \"*Ethnicity, Insurgency, and Civil War*\" by J Fearon and D Laitin explore factors that lead to civil wars.\n",
    "They try to find a link between civil conflicts and ethnicity. This paper argues that rather than recent changes in the post-Cold War world system, it is the steady growth in protracted crisis from the 1950s and 1960s onwards, that results in the current prevalence of civil wards. Additionally, issues such as poverty, political instability and weak states also contribute to the outbreak of civil wars as they favour rebel recruitment. According to this Stanford University paper, ethnicity, political grievances and the end of the Cold War have historically been cited as factors leading to civil wars. However, civil conflict can be viewed in terms of insurgency that is characterised by small, lightly armed groups engaged in guerilla warfare from rural bases. The existence of poverty, weak states, political instability and large populations leads to conditions that favour insurgency and recruitment to rebel groups.\n",
    "Fragile states are unable or unwilling to control internal conflicts due to weak local policing and corrupt counterinsurgency practices. Ethnic and religious diversity does not necessarily lead to civil conflict in a country, despite the fact that rebel groups are often mobilised along ethnic lines during warfare. They also argue that there is little evidence that the absence of democracy and respect for civil liberties and minority groups leads to the outbreak of civil war.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285dee6d",
   "metadata": {},
   "source": [
    "# b)\n",
    "\n",
    "In the section on **Descriptive Statistics** the authors mention they identified 127 conflicts between years 1945 to 1999. Each observation is a civil war conflict with 13 of those were anticolonial wars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f82f0",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311973a3",
   "metadata": {},
   "source": [
    "> **Model 1:**\n",
    "\n",
    "*Dependent variable*: Onset of Civil War (represented by 1 and absence of it by 0)\n",
    "\n",
    "*Independent variable*: \n",
    "\n",
    "* Prior war: -0.954**\n",
    "* Per capita income: -0.344***\n",
    "* log(population): 0.263***\n",
    "* log(% mountainous): 0.219**\n",
    "* Noncontiguous state: 0.443\n",
    "* Oil exporter: 0.858**\n",
    "* New state: 1.709***\n",
    "* Instability: 0.618**\n",
    "* Democracy$^{a,c}$: 0.021\n",
    "* Ethnic fractionalization: 0.166\n",
    "* Religious fractionalization: 0.285\n",
    "* Constant: -6.731***\n",
    "\n",
    "\n",
    "> **Model 2:**\n",
    "\n",
    "*Dependent variable*: Onset of \"Ethnic\" War (represented by 1 and absence of it by 0)\n",
    "\n",
    "*Independent variable*: \n",
    "\n",
    "* Prior war: -0.849*\n",
    "* Per capita income: -0.379***\n",
    "* log(population): -0.389***\n",
    "* log(% mountainous): 0.120\n",
    "* Noncontiguous state: 0.481\n",
    "* Oil exporter: 0.809*\n",
    "* New state: 1.777***\n",
    "* Instability: 0.385\n",
    "* Democracy$^{a,c}$: 0.013\n",
    "* Ethnic fractionalization: 0.146 \n",
    "* Religious fractionalization: 1.533*\n",
    "* Constant: -8.450***\n",
    "\n",
    "> **Model 3:**\n",
    "\n",
    "*Dependent variable*: Onset of Civil War (represented by 1 and absence of it by 0)\n",
    "\n",
    "*Independent variable*: \n",
    "\n",
    "* Prior war: -0.916*\n",
    "* Per capita income: 0.318***\n",
    "* log(population): 0.272***\n",
    "* log(% mountainous): 0.199*\n",
    "* Noncontiguous state: 0.426\n",
    "* Oil exporter: 0.751**\n",
    "* New state: 1.658***\n",
    "* Instability: 0.513*\n",
    "* Ethnic fractionalization: 0.164\n",
    "* Religious fractionalization: 0.326\n",
    "* Anocracy: 0.521*\n",
    "* Democracy$^{a,d}$: 0.127\n",
    "* Constant: -7.019***\n",
    "\n",
    "> **Model 4:**\n",
    "\n",
    "*Dependent variable*: Onset of Civil War (Plus Empired) (represented by 1 and absence of it by 0)\n",
    "\n",
    "*Independent variable*: \n",
    "\n",
    "* Prior war: -0.688**\n",
    "* Per capita income: -0.305***\n",
    "* log(population): 0.267***\n",
    "* log(% mountainous): 0.192*\n",
    "* Noncontiguous state: 0.798**\n",
    "* Oil exporter: 0.548*\n",
    "* New state: 1.523***\n",
    "* Instability: 0.548*\n",
    "* Ethnic fractionalization: 0.490\n",
    "* Constant: -6.801***\n",
    "\n",
    "> **Model 5:**\n",
    "\n",
    "*Dependent variable*: Onset of Civil War (COW) (represented by 1 and absence of it by 0)\n",
    "\n",
    "*Independent variable*: \n",
    "\n",
    "* Prior war: -0.551\n",
    "* Per capita income: -0.309***\n",
    "* log(population): 0.223**\n",
    "* log(% mountainous): 0.418***\n",
    "* Noncontiguous state : -0.171\n",
    "* Oil exporter: 1.269***\n",
    "* New state: 1.147*\n",
    "* Instability: 0.584*\n",
    "* Ethnic fractionalization: -0.119\n",
    "* Religious fractionalization: 1.176*\n",
    "* Anocracy: 0.597*\n",
    "* Democracy$^{a,d}$: 0.219\n",
    "* Constant: -7.503***\n",
    "\n",
    "---\n",
    "\n",
    "#### NOTES : \n",
    "\n",
    "Any attribute with a * or ** or *** is significant as the literature defines the stars as follows:**\n",
    "\n",
    "\\* => p-value < 0.05\n",
    "\n",
    "\\** => p-value < 0.01\n",
    "\n",
    "\\*** => p-value < 0.001\n",
    "\n",
    "and we know that p < 0.05 is usually seen as a significant value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb955fda",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc65a6",
   "metadata": {},
   "source": [
    "The coefficients in the table are the *weights* associated with each feature/independent variable for the model. The weights themselves have no meaning as these are **Logit Regression** models. However, raising the weights to an exponent gives us a value for **odds ratio**, i.e. \n",
    "$$O_i= e^{w_i}$$\n",
    "\n",
    "A change in the weight of attribute, $w_i$, will change (increase if the weight is positive and decrease if the weight is negative) the odds of onset of war vs no onset of war in a particular year (which is our dependent variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae908003",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11413f",
   "metadata": {},
   "source": [
    "***PLEASE REFER BACK TO (C) as it has all the variables and their associated signed values for each model. Also the note at the end answers for which attributes are significant.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5244d9",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756deae",
   "metadata": {},
   "source": [
    "At first glance of the weights, it seems that there is a large discrepancy between how much each of the attributes affect the dependent variable. But after a closer inspection, you can see that the scales of the independent variables vary to quite an extent. E.g. the **population** attribute is on a *log-scale* certain other attributes are *binary* and some are not in log-scale. This means that the weights do not entirely represent the impact of each variable. Obviously a variable on a log-scale has a very high range of possible values compared to other attributes. This means that the parametric equation we are trying to find to define our dependent variable must account for the large difference in range of the attributes. So, while the table associates a smaller weight to the log-scaled variables, it does so to ensure that certain variables do not completely overpower the rest. A better way to find out the relative impacts of the attributes would be normalize the data so that they all fall within the same range. \n",
    "\n",
    "In conclusion,\\\n",
    "it is difficult to say which of the attributes have a greater impact by looking at the weights given to us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73240",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9029ea4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:18.477289Z",
     "start_time": "2022-02-13T23:13:15.638844Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.typing import ArrayLike\n",
    "from random import randrange\n",
    "import random\n",
    "random.seed(265)\n",
    "import matplotlib\n",
    "font = {'family' : 'Tahoma',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : '20'}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('axes',edgecolor='lightgrey')\n",
    "matplotlib.rcParams['grid.color'] = 'lightgrey'\n",
    "plt.style.use('seaborn-pastel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dad8ab",
   "metadata": {},
   "source": [
    "<h1 align='center'>Q2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e0c34",
   "metadata": {},
   "source": [
    "### a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe39b6",
   "metadata": {},
   "source": [
    "### Matrices :\n",
    "\n",
    "\n",
    "X$_{n\\times m}$= $\\begin{bmatrix}\n",
    ".&.&.&. \\\\\n",
    ".&.&.&. \\\\\n",
    ".&.&.&. \\\\\n",
    ".&.&.&. \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "y$_{1\\times m}$= $\\begin{bmatrix}\n",
    ".&.&.&. \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\theta_{n\\times1}$=  $\\begin{bmatrix}\n",
    "\\theta_1 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "\\theta_n \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Equations:\n",
    "\n",
    "$\\sigma = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$h(\\theta) = \\sigma(\\theta^{T}X + b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0252e787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:18.490367Z",
     "start_time": "2022-02-13T23:13:18.479021Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_f(z: ArrayLike) -> ArrayLike:\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    z: ArrayLike\n",
    "        The argument of the sigmoid function\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    sigmoid: ArrayLike\n",
    "        The output of the sigmoid function\n",
    "    '''\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "def classifier_f(w: ArrayLike, b:int, X: ArrayLike) -> ArrayLike:\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    w: ArrayLike\n",
    "        Weights for each attribute\n",
    "    b: int\n",
    "        Bias value\n",
    "    X: Arraylike\n",
    "        Feature matrix\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    pred: ArrayLike\n",
    "        The output of the hypothesis function\n",
    "        \n",
    "    '''\n",
    "    z = np.dot(w.T, X) + b\n",
    "    pred = sigmoid_f(z)\n",
    "    return pred\n",
    "\n",
    "def binary_loss_f(y: ArrayLike, a: ArrayLike) -> int:\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    y: ArrayLike\n",
    "        Target vector\n",
    "    a: ArrayLike\n",
    "        Prediction vector from the classifier\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    cost: int\n",
    "        Value of the cross-entropy function\n",
    "    '''\n",
    "    m = y.shape[1]\n",
    "    cost = -1/m * np.sum(y * np.log(a) + (1-y) * np.log(1-a))\n",
    "    return cost\n",
    "\n",
    "def gradient_f(w: ArrayLike, b: int, X: ArrayLike, y: ArrayLike, alpha: int):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: ArrayLike\n",
    "        The weights that are to be optimized\n",
    "    \n",
    "    X: ArrayLike\n",
    "        Numpy Array of features\n",
    "    \n",
    "    y: ArrayLike\n",
    "        Numpy Array of targets\n",
    "\n",
    "    alpha: int\n",
    "        Used as a multiplier for the step-size\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    w: int\n",
    "        Returns the optimized values of the weights initially given as a parameter\n",
    "        \n",
    "    b: int\n",
    "        Returns the optimized value of the bias initially given as a parameter\n",
    "    \n",
    "    cost:\n",
    "        Returns the cost at the end of each iteration\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    A = classifier_f(w=w, b=b, X=X)\n",
    "    cost = binary_loss_f(y, A)\n",
    "    dw = (1/m) * np.dot(A-y, X.T)\n",
    "    db = (1/m) * np.sum(A-y)\n",
    "    w = w - alpha * dw.T\n",
    "    b = b - alpha * db\n",
    "    \n",
    "    return w, b, cost\n",
    "\n",
    "def optimizer_f(X: ArrayLike, y: ArrayLike, alpha: int, iterations: int, print_cost=True):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    w: ArrayLike\n",
    "        The weights that are to be optimized\n",
    "    \n",
    "    X: ArrayLike\n",
    "        Numpy Array of features\n",
    "    \n",
    "    y: ArrayLike\n",
    "        Numpy Array of targets\n",
    "\n",
    "    number_of_steps: int\n",
    "        Number of iterations for the gradient descent algorithm\n",
    "        \n",
    "    alpha: int\n",
    "        Used as a multiplier for the step-size\n",
    "    \n",
    "    print_cost: bool\n",
    "        True if the cost is to be printed \n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    w: int\n",
    "        Returns the optimized values of the weights initially given as a parameter\n",
    "        \n",
    "    b: int\n",
    "        Returns the optimized value of the bias initially given as a parameter\n",
    "    \n",
    "    cost_l:\n",
    "        Returns the list of the cost after all iterations\n",
    "    '''\n",
    "    X = X.T # m x n to n x m \n",
    "    y = y.reshape((1, X.shape[1]))\n",
    "    w = np.zeros((X.shape[0],1))\n",
    "    b = 0\n",
    "    cost_l = []\n",
    "    for i in range(iterations):\n",
    "        w, b, cost = gradient_f(w, b, X, y, alpha)\n",
    "        cost_l.append(cost)\n",
    "        if print_cost:\n",
    "            if i % (iterations/50) == 0:\n",
    "                print(f\"Cost after {i}-iterations: {cost}\")\n",
    "            \n",
    "    return w,b, cost_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9198904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:18.566697Z",
     "start_time": "2022-02-13T23:13:18.491796Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing data\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer(as_frame=True).frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1819348f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:18.596563Z",
     "start_time": "2022-02-13T23:13:18.568745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce2a347",
   "metadata": {},
   "source": [
    "### a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d1ec68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:18.600759Z",
     "start_time": "2022-02-13T23:13:18.597726Z"
    }
   },
   "outputs": [],
   "source": [
    "# splitting data into features and targets \n",
    "\n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266dbac0",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832e40b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:18.605185Z",
     "start_time": "2022-02-13T23:13:18.601849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling x only since y already in [0,1]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms_x = MinMaxScaler()\n",
    "x = mms_x.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1ec31",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92aab72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.421370Z",
     "start_time": "2022-02-13T23:13:18.606351Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 0-iterations: 0.6931471805599453\n",
      "Cost after 200-iterations: 0.2190089573081807\n",
      "Cost after 400-iterations: 0.16977855890618357\n",
      "Cost after 600-iterations: 0.1471224255623226\n",
      "Cost after 800-iterations: 0.13320273075450312\n",
      "Cost after 1000-iterations: 0.1235070099271086\n",
      "Cost after 1200-iterations: 0.11625789315691809\n",
      "Cost after 1400-iterations: 0.11058179508064696\n",
      "Cost after 1600-iterations: 0.1059883755798705\n",
      "Cost after 1800-iterations: 0.10217707737224219\n",
      "Cost after 2000-iterations: 0.09895175654648952\n",
      "Cost after 2200-iterations: 0.09617823688344139\n",
      "Cost after 2400-iterations: 0.09376128095991756\n",
      "Cost after 2600-iterations: 0.09163123853205997\n",
      "Cost after 2800-iterations: 0.08973589147353446\n",
      "Cost after 3000-iterations: 0.08803525864865586\n",
      "Cost after 3200-iterations: 0.08649816919210591\n",
      "Cost after 3400-iterations: 0.08509993487791848\n",
      "Cost after 3600-iterations: 0.08382072870895182\n",
      "Cost after 3800-iterations: 0.08264443041624471\n",
      "Cost after 4000-iterations: 0.08155778839419911\n",
      "Cost after 4200-iterations: 0.08054980081380513\n",
      "Cost after 4400-iterations: 0.07961125151318411\n",
      "Cost after 4600-iterations: 0.07873435709692277\n",
      "Cost after 4800-iterations: 0.07791249519799844\n",
      "Cost after 5000-iterations: 0.07713999281994341\n",
      "Cost after 5200-iterations: 0.07641195973249713\n",
      "Cost after 5400-iterations: 0.07572415605572207\n",
      "Cost after 5600-iterations: 0.07507288607282177\n",
      "Cost after 5800-iterations: 0.07445491236935119\n",
      "Cost after 6000-iterations: 0.07386738587292113\n",
      "Cost after 6200-iterations: 0.07330778843999079\n",
      "Cost after 6400-iterations: 0.07277388542428678\n",
      "Cost after 6600-iterations: 0.07226368624642869\n",
      "Cost after 6800-iterations: 0.07177541142301842\n",
      "Cost after 7000-iterations: 0.07130746484543367\n",
      "Cost after 7200-iterations: 0.07085841035198491\n",
      "Cost after 7400-iterations: 0.07042695183211982\n",
      "Cost after 7600-iterations: 0.07001191625260525\n",
      "Cost after 7800-iterations: 0.06961223911375602\n",
      "Cost after 8000-iterations: 0.06922695193670049\n",
      "Cost after 8200-iterations: 0.0688551714562228\n",
      "Cost after 8400-iterations: 0.06849609025231031\n",
      "Cost after 8600-iterations: 0.06814896860046728\n",
      "Cost after 8800-iterations: 0.06781312735866418\n",
      "Cost after 9000-iterations: 0.06748794173941769\n",
      "Cost after 9200-iterations: 0.06717283584041388\n",
      "Cost after 9400-iterations: 0.06686727782747727\n",
      "Cost after 9600-iterations: 0.06657077568043157\n",
      "Cost after 9800-iterations: 0.06628287342622179\n"
     ]
    }
   ],
   "source": [
    "iterations = 10000\n",
    "w, b, costs = optimizer_f(x, y, alpha=0.5, iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e3483f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.602009Z",
     "start_time": "2022-02-13T23:13:19.422624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.arange(0, iterations, 1), costs)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2f4353",
   "metadata": {},
   "source": [
    "### e, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7eac8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.608996Z",
     "start_time": "2022-02-13T23:13:19.603371Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = (np.ravel(w))\n",
    "attrNames = dataset.columns[:-1]\n",
    "oddsRatio = np.exp(np.ravel(w))\n",
    "eq = np.stack((weights, attrNames, oddsRatio), axis=1)\n",
    "eq_df = pd.DataFrame(data=eq, index=None, columns=['Weights', 'Attribute Name', 'Odds Ratio'])\n",
    "eq_df.sort_values(by='Weights', ascending=False, inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33665711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.619927Z",
     "start_time": "2022-02-13T23:13:19.611837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights</th>\n",
       "      <th>Attribute Name</th>\n",
       "      <th>Odds Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.023169</td>\n",
       "      <td>mean fractal dimension</td>\n",
       "      <td>55.877883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.874137</td>\n",
       "      <td>compactness error</td>\n",
       "      <td>48.141124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.441101</td>\n",
       "      <td>fractal dimension error</td>\n",
       "      <td>11.485683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.323793</td>\n",
       "      <td>mean compactness</td>\n",
       "      <td>3.757646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.298801</td>\n",
       "      <td>symmetry error</td>\n",
       "      <td>3.664901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.216559</td>\n",
       "      <td>concavity error</td>\n",
       "      <td>3.375554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.430571</td>\n",
       "      <td>texture error</td>\n",
       "      <td>1.538135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087119</td>\n",
       "      <td>mean symmetry</td>\n",
       "      <td>1.091026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.03104</td>\n",
       "      <td>concave points error</td>\n",
       "      <td>1.031526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.199341</td>\n",
       "      <td>mean smoothness</td>\n",
       "      <td>0.819271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.348645</td>\n",
       "      <td>worst compactness</td>\n",
       "      <td>0.705643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.771565</td>\n",
       "      <td>worst fractal dimension</td>\n",
       "      <td>0.462289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.904273</td>\n",
       "      <td>smoothness error</td>\n",
       "      <td>0.404836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.311277</td>\n",
       "      <td>mean radius</td>\n",
       "      <td>0.269476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.393625</td>\n",
       "      <td>mean perimeter</td>\n",
       "      <td>0.248174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.62534</td>\n",
       "      <td>mean area</td>\n",
       "      <td>0.072415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.182783</td>\n",
       "      <td>worst concavity</td>\n",
       "      <td>0.04147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-3.343976</td>\n",
       "      <td>mean texture</td>\n",
       "      <td>0.035296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-3.839267</td>\n",
       "      <td>worst smoothness</td>\n",
       "      <td>0.021509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-3.890705</td>\n",
       "      <td>area error</td>\n",
       "      <td>0.020431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-4.149751</td>\n",
       "      <td>worst symmetry</td>\n",
       "      <td>0.015768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-4.412854</td>\n",
       "      <td>worst perimeter</td>\n",
       "      <td>0.012121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-4.553091</td>\n",
       "      <td>perimeter error</td>\n",
       "      <td>0.010535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-4.573361</td>\n",
       "      <td>mean concavity</td>\n",
       "      <td>0.010323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-4.836219</td>\n",
       "      <td>worst area</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-5.150528</td>\n",
       "      <td>worst concave points</td>\n",
       "      <td>0.005796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-5.190218</td>\n",
       "      <td>worst radius</td>\n",
       "      <td>0.005571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-5.433743</td>\n",
       "      <td>worst texture</td>\n",
       "      <td>0.004367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-6.286055</td>\n",
       "      <td>mean concave points</td>\n",
       "      <td>0.001862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-6.321238</td>\n",
       "      <td>radius error</td>\n",
       "      <td>0.001798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weights           Attribute Name Odds Ratio\n",
       "0   4.023169   mean fractal dimension  55.877883\n",
       "1   3.874137        compactness error  48.141124\n",
       "2   2.441101  fractal dimension error  11.485683\n",
       "3   1.323793         mean compactness   3.757646\n",
       "4   1.298801           symmetry error   3.664901\n",
       "5   1.216559          concavity error   3.375554\n",
       "6   0.430571            texture error   1.538135\n",
       "7   0.087119            mean symmetry   1.091026\n",
       "8    0.03104     concave points error   1.031526\n",
       "9  -0.199341          mean smoothness   0.819271\n",
       "10 -0.348645        worst compactness   0.705643\n",
       "11 -0.771565  worst fractal dimension   0.462289\n",
       "12 -0.904273         smoothness error   0.404836\n",
       "13 -1.311277              mean radius   0.269476\n",
       "14 -1.393625           mean perimeter   0.248174\n",
       "15  -2.62534                mean area   0.072415\n",
       "16 -3.182783          worst concavity    0.04147\n",
       "17 -3.343976             mean texture   0.035296\n",
       "18 -3.839267         worst smoothness   0.021509\n",
       "19 -3.890705               area error   0.020431\n",
       "20 -4.149751           worst symmetry   0.015768\n",
       "21 -4.412854          worst perimeter   0.012121\n",
       "22 -4.553091          perimeter error   0.010535\n",
       "23 -4.573361           mean concavity   0.010323\n",
       "24 -4.836219               worst area   0.007937\n",
       "25 -5.150528     worst concave points   0.005796\n",
       "26 -5.190218             worst radius   0.005571\n",
       "27 -5.433743            worst texture   0.004367\n",
       "28 -6.286055      mean concave points   0.001862\n",
       "29 -6.321238             radius error   0.001798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8944c",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://medium.com/@mubarakb/how-to-interpret-the-weights-in-logistic-regression-89bb03249f27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0271b5",
   "metadata": {},
   "source": [
    "A change in the weight of an attribute, $w_i$, will change (increase if the weight is positive and decrease if the weight is negative) the odds of breast cancer vs no breast cancer by the odds ratio, $O_i$ associated to the weight of the attribute, $w_i$.\n",
    "\n",
    "$$O_i = e^{w_i}$$\n",
    "\n",
    "E.g. An increase in **mean fractal dimension** will **increase** the odds of having cancer vs not having cancer by a whopping \"55.877883\" times. While an increase in **worst texture** will **decrease** the odds of having cancer over not having cancer by \"0.004367\" times.\n",
    "\n",
    "We can interpret all the weights in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71209852",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742506cf",
   "metadata": {},
   "source": [
    "<h1 align='center'>Q3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bacad0",
   "metadata": {},
   "source": [
    "### a) Implementation of the CV methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bf96498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.708026Z",
     "start_time": "2022-02-13T23:13:19.622390Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def LOOCV(dataset: ArrayLike, model, X: ArrayLike, y: ArrayLike):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset:\n",
    "        A pandas dataframe\n",
    "    \n",
    "    model:\n",
    "        A linear regression model\n",
    "    \n",
    "    X: ArrayLike\n",
    "        An array of the feature names in the dataset\n",
    "        \n",
    "    y: ArrayLike\n",
    "        An array of the target names in the dataset\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        The result of K-Fold CV with k=len(dataset)\n",
    "    '''\n",
    "    assert isinstance(model, sklearn.linear_model.LinearRegression), \"Your model must be an instance of sklearn.linear_model.LinearRegression\"\n",
    "    \n",
    "    return KFCV(dataset, model, X, y, k=len(dataset))\n",
    "\n",
    "\n",
    "def KFCV(dataset: ArrayLike, model, X: ArrayLike, y: ArrayLike, k: int = 5):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset:\n",
    "        A pandas dataframe\n",
    "    \n",
    "    model:\n",
    "        A linear regression model\n",
    "    \n",
    "    X: ArrayLike\n",
    "        An array of the feature names in the dataset\n",
    "        \n",
    "    y: ArrayLike\n",
    "        An array of the target names in the dataset\n",
    "    \n",
    "    k: int\n",
    "        The number of folds that the dataset is to be subdivided into\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    model_dict: dict\n",
    "        A dictionary with the keys as the mean square error of the model and the values as the models\n",
    "    '''\n",
    "    assert isinstance(model, sklearn.linear_model.LinearRegression), \"Your model must be an instance of sklearn.linear_model.LinearRegression\"\n",
    "    \n",
    "    model_dict = {}\n",
    "    dataset = shuffle(dataset, random_state=265)\n",
    "    folds = len(dataset) // k\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        # splitting into train-test\n",
    "        test = dataset.iloc[i*folds:(i+1)*folds, :]\n",
    "        train = pd.concat([dataset.iloc[:i*folds, :], dataset.iloc[(i+1)*folds: , :]])\n",
    "        \n",
    "        # fitting the model\n",
    "        model = model.fit(train[X], train[y])\n",
    "        \n",
    "        # finding mse \n",
    "        err = mean_squared_error(y_true=test[y], y_pred=model.predict(test[X]))\n",
    "        \n",
    "        model_dict[err] = model\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "def TTSCV(dataset: ArrayLike, model, X: ArrayLike, y: ArrayLike, split_split: float = 0.7):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset:\n",
    "        A pandas dataframe\n",
    "    \n",
    "    model:\n",
    "        A linear regression model\n",
    "    \n",
    "    X: ArrayLike\n",
    "        An array of the feature names in the dataset\n",
    "        \n",
    "    y: ArrayLike\n",
    "        An array of the target names in the dataset\n",
    "    \n",
    "    split: float\n",
    "        The fraction of the dataset that is to be the train set\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    err: float\n",
    "        The mean square error of the model\n",
    "    '''\n",
    "    assert isinstance(model, sklearn.linear_model.LinearRegression), \"Your model must be an instance of sklearn.linear_model.LinearRegression\"\n",
    "    \n",
    "    dataset = shuffle(dataset, random_state=265)\n",
    "    train_size = int(split_split * len(dataset))\n",
    "    \n",
    "    # splitting into train-test\n",
    "    train = dataset.sample(n=train_size, random_state=265)\n",
    "    test = dataset.drop(train.index, axis=0)\n",
    "    \n",
    "    # fitting the model\n",
    "    \n",
    "    model = model.fit(train[X], train[y])\n",
    "\n",
    "    # finding mse \n",
    "    err = mean_squared_error(y_true=test[y], y_pred=model.predict(test[X]))\n",
    "    \n",
    "    return err\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3c26b",
   "metadata": {},
   "source": [
    "### b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62362f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.725710Z",
     "start_time": "2022-02-13T23:13:19.709118Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing california housing data from sklearn \n",
    "\n",
    "housing_df = sklearn.datasets.fetch_california_housing(as_frame=True).frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "866d35c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.736444Z",
     "start_time": "2022-02-13T23:13:19.727021Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaling the entire dataframe\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "mms.fit(housing_df)\n",
    "df = mms.fit_transform(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b3012bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.740026Z",
     "start_time": "2022-02-13T23:13:19.737644Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting scaled df into a dataframe\n",
    "df = pd.DataFrame(df, columns=housing_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3e9d2a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.744005Z",
     "start_time": "2022-02-13T23:13:19.741138Z"
    }
   },
   "outputs": [],
   "source": [
    "# selecting features and targets\n",
    "x = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54c01cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:13:19.748181Z",
     "start_time": "2022-02-13T23:13:19.745979Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializing model\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01be92c",
   "metadata": {},
   "source": [
    "### e, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99b720e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:15:42.503474Z",
     "start_time": "2022-02-13T23:13:19.749475Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tts_mse = TTSCV(df, model, x.columns, y.name)\n",
    "kf_mse = KFCV(df, model, x.columns, y.name)\n",
    "llo_mse = LOOCV(df, model, x.columns, y.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e79189fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T23:15:42.513889Z",
     "start_time": "2022-02-13T23:15:42.506475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test CV: 0.02234982261389473\n",
      "K-fold CV: 0.02246052322536991\n",
      "Leave One Out CV: 0.022456875235560305\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train-test CV: {tts_mse}\\nK-fold CV: {np.mean(list(kf_mse.keys()))}\\nLeave One Out CV: {np.mean(list((llo_mse.keys())))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da1170",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Above we have implemented three different cross-validation techniques:\n",
    "\n",
    "> Train-Test: This method partitions the dataset into **two sub-datasets** - the test and the train set. The model then learns using the training set and we test the model on \"new\" data from the test set. Finally we calculate the mean squared error associated to the model.\n",
    "\n",
    "> K-fold: This method partitions the original dataset into **k sub-datasets**. The model then learns using the k-1 of the subsets and we test the model on \"new\" data from the other one subset. Finally we iterate this process k times and calculate the mean squared error associated to the k different models. Then we return some aggregiate form (in our case the mean) of all the mean squared errors.\n",
    "\n",
    "> Leave One Out: This method is similar to the K-fold method except here we divide the dataset into **n sub-datasets** with n being the length of the dataset/the number of entries in the dataset.\n",
    "\n",
    "From all these methods I get that the values:\n",
    "\n",
    "**Train-test CV**: 0.02234982261389473\n",
    "\n",
    "**K-fold CV**: 0.02246052322536991\n",
    "\n",
    "**Leave One Out CV**: 0.022456875235560305\n",
    "\n",
    "So from my analysis, all three methods give very comparable results. However, it appears Train-Test Cross Validation ***marginally*** gives the lowest error relative to the others followed by LLOCV. This is a little surprising at first because one could naively assume that Leave One Out Cross Validation would give the lowest error. However, it might be because of certain outliers that may have caused the mean of LLOCV to be higher than K-Fold CV as the LLOCV does many scans of the entire database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4c359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
